{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb968bc4-3cbe-4cf2-aa79-6ceb66c97d16",
   "metadata": {},
   "source": [
    "<h1>Let's see what's the transformer inside...</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe4830-a4b8-4e09-9c2d-acd10004103a",
   "metadata": {},
   "source": [
    "<h2>Attention is all you need</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb1df8-9d24-4c93-9a97-12657c4e8983",
   "metadata": {},
   "source": [
    "* The core behind the Transformer model is the attention mechanism\n",
    "* The intuition behind attention is that rather than compressing the input, it might be better for the model to revisit the input sequence at every it's logical part.\n",
    "* Rather than always seeing the same representation of the input, one might imagine that the decoder should selectively focus on particular parts of the input sequence at particular decoding steps.\n",
    "* The encoder could produce a representation of length equal to the original input sequence. Then, at decoding time, the decoder can (via some control mechanism) receive as input a context vector consisting of a weighted sum of the representations on the input at each part. Intuitively, the weights determine the extent to which each step’s context “focuses” on each input token, and the key is to make this process for assigning the weights differentiable so that it can be learned along with all of the other neural network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30679e79-2af8-422a-a1b2-c61d44aff44b",
   "metadata": {},
   "source": [
    "<h2>But what does Attention actually do?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5d474-e2a4-4ee8-8d3e-70879e5da6a4",
   "metadata": {},
   "source": [
    "Let's breakdown the simple example. Cosider we're having a dict of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a61065ce-a66d-4d93-8f17-38b75fe90f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"apple\": \"green\",\n",
    "    \"banana\": \"yellow\",\n",
    "    \"orange\": \"orange\",\n",
    "    \"pickles\": \"green\",\n",
    "    \"pineapple\": \"brown\",\n",
    "    \"raspberry\": \"crimson\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43e80726-53e6-4506-bc50-9c4eb72de97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name -> height\n",
    "d = {\n",
    "    \"Ann\": 167,\n",
    "    \"Mike\": 186,\n",
    "    \"Michelle\": 170,\n",
    "    \"Andrew\": 176\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1466ba9-8d50-432e-8f10-545b63eeca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d[\"Anna\"]\n",
    "\n",
    "# \"Anna\": 0.98 * d[\"Ann\"] + 0.05 * d[\"Mike\"] + 0.05 * d[\"Michelle\"] + 0.1 * d[\"Andrew\"] = ...\n",
    "# \\alpha(q, k_i) * v_i\n",
    "\n",
    "# \\alpha(q, k_i) = 1 / len(d)\n",
    "# \\sum (\\alpha(q, k_i) * v_i )\n",
    "# 1/4 * 167 + 1/4 * 186 + 1/4 * 170 + 1/4 * 176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d9c83-9b53-4990-9e76-8f082fbd473e",
   "metadata": {},
   "source": [
    "This dict $D$ has keys $k$ and values $v$. We can operate on $D$, for instance with the exact query $q$ for “pickles” which would return the value “green”. If “pickles”:\"green\" was not a record in $D$, there would be no valid answer. If we also allowed for approximate matches, we would retrieve (“pineapple”, “brown”) instead. This quite simple and trivial example nonetheless teaches us a number of useful things:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e76ce2-92f1-4a9d-b774-2e0b4481136f",
   "metadata": {},
   "source": [
    "* We can design queries $q$ that operate on ($k$, $v$) pairs in such a manner as to be valid regardless of the database size.\n",
    "\n",
    "* The same query can receive different answers, according to the contents of the database.\n",
    "* The “code” being executed for operating on a large state space (the database) can be quite simple (e.g., exact match, approximate match, top-k\n",
    ").\n",
    "\n",
    "* There is no need to compress or simplify the database to make the operations effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43d54a-d345-4132-aa36-79fe1017c8e8",
   "metadata": {},
   "source": [
    "For example, we would like to design something which will define the \"closiness\" of the query $q$ to the keys $k$ in $D$. Let's suppose we are having a function $\\alpha(q, k): \\mathbb{R}^{[2, n]->\\mathbb{R}^n}$ which calculates some sort of key-query similarity. E.g, for $q=\\text{\"pi\"}$ it could be something like $(0.05,\\ 0.075,\\ 0.075,\\ 0.3,\\ 0.4,\\ 0.1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17278081-4d29-417e-b154-a1ccdae7545c",
   "metadata": {},
   "source": [
    "Also we want to associate this similarity with the values in the dataset. For example, we can multiply those values above by the values in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d9cf8-66d6-44bc-b397-cbd56ebe36f6",
   "metadata": {},
   "source": [
    "This gives us a basic intuition what the attention actually is:\n",
    "\n",
    "$$\\text{A} = \\sum\\limits_{d\\in D}{\\alpha(q, k_d)*v_d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca68c4-1aae-48a6-b824-521d53218531",
   "metadata": {},
   "source": [
    "The operation itself is typically referred to as attention pooling. The name attention derives from the fact that the operation pays particular attention to the terms for which the $\\alpha$ weight \n",
    " is significant (i.e., large). As such, the attention over $D$\n",
    " generates a linear combination of values contained in the database. This gives us the next situations:\n",
    "\n",
    "* The weights $\\alpha(q, k) \\geq 0$ : in this case the output of the attention mechanism is contained in the convex cone spanned by\n",
    "the values $v$.\n",
    "\n",
    "* The weights  $\\alpha(q, k) \\geq 0$ \n",
    " form a convex combination $\\sum\\limits_{d\\in D}{\\alpha(q, k_d)}=1$,\n",
    ". This is the most common setting in deep learning.\n",
    "\n",
    "* Exactly one of the weights \n",
    " is 1, while all others are 0\n",
    ". This is akin to a traditional database query.\n",
    "\n",
    "* All weights are equal: this amounts to averaging across the entire database, also called average pooling in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72131ed5-5a66-451f-94d1-3d045613c3fd",
   "metadata": {},
   "source": [
    "A common strategy for ensuring that the weights sum up to \n",
    " is to 1 normalize them via\n",
    "\n",
    "$$\\alpha(q, k_i) = \\frac{\\alpha(q, k_i)}{\\sum_i\\alpha(q, k_i)}$$\n",
    " \n",
    "In particular, to ensure that the weights are also nonnegative, one can resort to exponentiation. This means that we can now pick any function \n",
    " and then apply the softmax operation used for multinomial models to it via\n",
    "\n",
    "$$\\alpha(q, k_i) = \\frac{\\exp a(q, k_i)}{\\sum_i\\exp a(q, k_i)}$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59263742-13fc-470b-aaf5-bf63fba9c835",
   "metadata": {},
   "source": [
    "![title](qkv.svg)\n",
    "![title](attention-output.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2187b-8850-42f2-99dc-dfacd44e7ed8",
   "metadata": {},
   "source": [
    "<h2>Let's code one of them: scaled dot product</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0de41-39a9-4b61-afb9-32bd38c8d6ac",
   "metadata": {},
   "source": [
    "Both the query and the key have the same vector length, say \n",
    "$d$, even though this can be addressed easily by replacing $q^T k$\n",
    " with $q^T Mk$\n",
    " where $M$\n",
    " is a matrix suitably chosen for translating between both spaces. For now assume that the dimensions match.\n",
    "\n",
    "In practice, we often think of minibatches for efficiency, such as computing attention for \n",
    " queries and \n",
    " key-value pairs, where queries and keys are of length \n",
    " and values are of length \n",
    ". The scaled dot product attention of queries $Q\\in R^{n\\times d}$\n",
    ", keys $K\\in R^{m\\times d}$\n",
    ", and values $V\\in R^{m\\times v}$\n",
    " thus can be written as\n",
    " $$\\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d}}\\Big)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b569e341-8b89-4202-917a-c7fe7af8eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b159343e-86c0-4f9e-b3ec-398fae984814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # queries: (B, N, D)\n",
    "        # keys: (B, M, D)\n",
    "        # values: (B, M, V)\n",
    "\n",
    "        assert queries.shape[0] == keys.shape[0]\n",
    "        assert queries.shape[0] == values.shape[0]\n",
    "        assert queries.shape[-1] == keys.shape[-1]\n",
    "        assert keys.shape[1] == values.shape[1]\n",
    "\n",
    "        d = keys.shape[-1]\n",
    "\n",
    "        # scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        scores = queries @ keys.transpose(1, 2) / np.sqrt(d)\n",
    "        attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        return attention_weights @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc4258a-e1b5-439e-86ad-4b4678ea7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.zeros((2, 1, 2))\n",
    "keys = torch.zeros((2, 10, 2))\n",
    "values = torch.zeros((2, 10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d37c5880-8f9e-4cd8-b141-b0844b34782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = DotProductAttention()\n",
    "att(queries, keys, values).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2fdf8-6995-414f-8f1d-d4f9dbee0f20",
   "metadata": {},
   "source": [
    "<h2>Multihead attention</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a33e90-771f-4d47-baed-e7324b0f9745",
   "metadata": {},
   "source": [
    "Before providing the implementation of multi-head attention, let’s formalize this model mathematically. Given a query \n",
    "$q\\in \\mathbb{R}^{d_q}$, a key $ q\\in \\mathbb{R}^{d_k}$\n",
    ", and a value $v\\in \\mathbb{R}^{d_v}$\n",
    ", each attention head $h_i$\n",
    "is computed as\n",
    "$$h_i = f(W_i^q q, W_i^k k, W_i^v v)$$ \n",
    "\n",
    "where $W_i^q \\in \\mathbb{R}^{p_q \\times d_q}, W_i^k \\in \\mathbb{R}^{p_k \\times d_k}, W_i^v \\in \\mathbb{R}^{p_v \\times d_v}$\n",
    "\n",
    " are learnable parameters and $f$\n",
    " is attention pooling, such as additive attention and scaled dot product attention. The multi-head attention output is another linear transformation via learnable parameters $W_o \\in \\mathbb{R}^{p_o \\times hd_v}$\n",
    " of the concatenation of $h$\n",
    " heads:\n",
    "\n",
    "$$W_o [h_1, ..., h_n]^T \\in \\mathbb{R^{p_o}}$$\n",
    " \n",
    " \n",
    "Based on this design, each head may attend to different parts of the input. More sophisticated functions than the simple weighted average can be expressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646b454-dc89-44e9-ab5f-b00bea59e6f5",
   "metadata": {},
   "source": [
    "To avoid significant growth of computational cost and parametrization cost, lets set \n",
    "$p_q = p_k = p_v = p_o / h$\n",
    ". Note that \n",
    " heads can be computed in parallel if we set the number of outputs of linear transformations for the query, key, and value to \n",
    "$p_q h = p_k h = p_v h = p_o$. In the following implementation, \n",
    " is specified via the argument num_hiddens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c7e6811-eb84-4792-9a76-6eac385f3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_hiddens = num_hiddens\n",
    "\n",
    "        self.attention = DotProductAttention()\n",
    "\n",
    "        self.W_q = nn.LazyLinear(num_hiddens)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens)\n",
    "\n",
    "        self.W_o = nn.LazyLinear(num_hiddens)\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "        # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "    \n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # qkv: (B, N_qkv, num_hiddens)\n",
    "\n",
    "        q = self.transpose_qkv(self.W_q(queries))\n",
    "        k = self.transpose_qkv(self.W_k(keys))\n",
    "        v = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        output = self.transpose_output(self.attention(q, k, v))\n",
    "\n",
    "        return self.W_o(output)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e6d7254-e994-45a1-b416-106935a67dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 100\n",
    "num_heads = 5\n",
    "\n",
    "mha = MultiHeadAttention(num_hiddens, num_heads)\n",
    "\n",
    "X = torch.ones((16, 5, 5))\n",
    "mha(X, X, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb4991-0322-4b16-aa17-bf8290771f6d",
   "metadata": {},
   "source": [
    "<h2>Positional encoding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144e825-0079-4bc0-a2cf-eceb13f9a591",
   "metadata": {},
   "source": [
    "The dominant approach for preserving information about the order of tokens is to represent this to the model as an additional input associated with each token. These inputs are called positional encodings, and they can either be learned or fixed a priori. We now describe a simple scheme for fixed positional encodings based on sine and cosine functions (Vaswani et al., 2017).\n",
    "\n",
    "Suppose that the input representation $X \\in \\mathbb{R}^{n\\times d}$\n",
    " contains the \n",
    "d-dimensional embeddings for \n",
    " tokens of a sequence. The positional encoding outputs $X + P$\n",
    " using a positional embedding matrix $P$\n",
    " of the same shape, whose element on the \n",
    " i-th row and the 2j\n",
    " or the 2j+1  \n",
    " column is\n",
    " $$ p_{i, 2j} = \\sin \\frac{i}{10000^{2j/d}} $$\n",
    " $$ p_{i, 2j+1} = \\cos \\frac{i}{10000^{2j+1/d}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14ba6ad2-3ae7-41d9-8924-52abefe45fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, max_tokens=1000):\n",
    "        super().__init__()\n",
    "        self.P = torch.zeros((1, max_tokens, num_hiddens))\n",
    "        X = torch.arange(max_tokens, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32)/num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30959d50-65e6-4b14-88ed-b7b24e638352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 32]) torch.Size([1, 60, 32])\n"
     ]
    }
   ],
   "source": [
    "encoding_dim, num_steps = 32, 60\n",
    "pe = PositionalEncoding(encoding_dim)\n",
    "X = torch.zeros((1, num_steps, encoding_dim))\n",
    "XPE = pe(X)\n",
    "\n",
    "print(X.shape, XPE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f040803b-adec-4885-aff6-d5a6916138bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n",
       "           1.7783e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n",
       "           3.5566e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 4.3616e-01,  8.9987e-01,  5.9521e-01,  ...,  9.9984e-01,\n",
       "           1.0136e-02,  9.9995e-01],\n",
       "         [ 9.9287e-01,  1.1918e-01,  9.3199e-01,  ...,  9.9983e-01,\n",
       "           1.0314e-02,  9.9995e-01],\n",
       "         [ 6.3674e-01, -7.7108e-01,  9.8174e-01,  ...,  9.9983e-01,\n",
       "           1.0492e-02,  9.9994e-01]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7f373-4875-41c1-8e6d-c810ecfe722f",
   "metadata": {},
   "source": [
    "<h2>Bringing everything together</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c62824-3e08-4f43-8b23-36307fb7cf81",
   "metadata": {},
   "source": [
    "![title](transformer.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "865563d3-4e02-4cb1-bb74-6df03961941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, ffn_dim, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.LazyLinear(ffn_dim)\n",
    "        self.w2 = nn.LazyLinear(num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.w2(self.relu(self.w1(X)))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, res, X):\n",
    "        return self.ln(X + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03df506f-5916-4a5f-bdf1-3d9ca317f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_dim, num_heads):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads)\n",
    "        self.ffn = FFN(ffn_dim, num_hiddens)\n",
    "        self.addnorm1 = AddNorm(num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26b120e5-1c30-4028-a235-b2c83efd5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_blocks, ffn_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = num_hiddens\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.positional_encoding  = PositionalEncoding(num_hiddens)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(TransformerEncoderBlock(num_hiddens, ffn_dim, num_heads))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            X = block(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "901d6a6a-1c1d-40e9-bfbb-2123479c2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(1000, 300, 6, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7e11fe5-7184-4393-bec9-9e078886f134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 300])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.zeros((2, 100)).long()\n",
    "encoder(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a355a90-4b36-4e8e-9704-9c81a91a92ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3ecd352-cea1-491c-ba8c-1c53b4d4721d",
   "metadata": {},
   "source": [
    "<h2>Your supertask!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4904389-345a-487e-8461-7ff73f04cd52",
   "metadata": {},
   "source": [
    "Using this encoder implementation try to implement ViT. Have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c26655f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patcher(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def forward(self, image):\n",
    "        b, c, h, w = image.shape\n",
    "\n",
    "        assert h % self.patch_size == 0 and w % self.patch_size == 0\n",
    "\n",
    "        patches = image.unfold(-2, self.patch_size, self.patch_size).unfold(-2, self.patch_size, self.patch_size)\\\n",
    "            .reshape(b, c, -1, self.patch_size ** 2).transpose(1, 2).reshape(b, -1, c * self.patch_size ** 2)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e469046a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patcher = Patcher(16)\n",
    "img = torch.rand((2, 3, 256, 256))\n",
    "patcher(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "808a2bf6-6896-4d5a-8e38-e8ad7f5e2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, patch_size, channels, num_blocks, ffn_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = patch_size ** 2 * channels\n",
    "        \n",
    "        self.patcher = Patcher(patch_size)\n",
    "        self.positional_encoding = PositionalEncoding(self.num_hiddens)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(TransformerEncoderBlock(self.num_hiddens, ffn_dim, num_heads))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patcher(X)\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            X = block(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c656d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ViTEncoder(16, 3, 6, 100, 6)\n",
    "inp = torch.rand((2, 3, 256, 256))\n",
    "encoder(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bea0915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name: str, **kwargs):\n",
    "    activations = {\n",
    "        'relu': nn.ReLU,\n",
    "        'tanh': nn.Tanh,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "        'silu': nn.SiLU,\n",
    "        'softplus': nn.Softplus,\n",
    "        'leakyrelu': nn.LeakyReLU\n",
    "    }\n",
    "    if name in activations.keys():\n",
    "        return activations[name.lower()](**kwargs)\n",
    "    else:\n",
    "        raise KeyError('No such activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46f074e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTHead(nn.Module):\n",
    "    def __init__(self, neurons: List[int], activation: str = 'relu', dropout_rate: int = 0):\n",
    "        super().__init__()\n",
    "        self.n_layers = len(neurons)\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for n in neurons[:-1]:\n",
    "            self.blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.LazyLinear(n),\n",
    "                    nn.BatchNorm1d(n),\n",
    "                    get_activation(activation)\n",
    "                ))\n",
    "        self.blocks.append(\n",
    "            nn.Sequential(\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.LazyLinear(neurons[-1])\n",
    "            ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "001b3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            patch_size,\n",
    "            channels,\n",
    "            num_blocks,\n",
    "            ffn_dim,\n",
    "            num_heads,\n",
    "            head_neurons: List[int],\n",
    "            head_activation: str = 'relu',\n",
    "            head_dropout_rate: int = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ViTEncoder(patch_size, channels, num_blocks, ffn_dim, num_heads)\n",
    "        self.head = ViTHead(head_neurons, head_activation, head_dropout_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.encoder(X)\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        return self.head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f335db07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(16, 3, 6, 100, 6, [1000, 1000, 10])\n",
    "img = torch.rand((2, 3, 256, 256))\n",
    "model(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d55a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
