{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name: str, **kwargs):\n",
    "    activations = {\n",
    "        'relu': nn.ReLU,\n",
    "        'tanh': nn.Tanh,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "        'silu': nn.SiLU,\n",
    "        'softplus': nn.Softplus,\n",
    "        'leakyrelu': nn.LeakyReLU\n",
    "    }\n",
    "    if name in activations.keys():\n",
    "        return activations[name.lower()](**kwargs)\n",
    "    else:\n",
    "        raise KeyError('No such activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 5, 5])\n",
      "torch.Size([1, 4, 5, 5])\n",
      "torch.Size([1, 2, 10, 10])\n",
      "torch.Size([1, 2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, type: str, in_channels: int, out_channels: Optional[int] = None, mid_channels: Optional[int] = None, activations: List[str] | str = 'relu', kernel_size: int = 3, stride: int = 1, padding: int = 1) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        n_layers = 2\n",
    "        \n",
    "        if not isinstance(activations, List):\n",
    "            activations = [activations] * n_layers\n",
    "        if len(activations) != n_layers:\n",
    "            raise Exception('Not enough activations')\n",
    "        \n",
    "        match type:\n",
    "            case 'Standard':\n",
    "                mid_channels = mid_channels or in_channels\n",
    "                out_channels = out_channels or in_channels\n",
    "                \n",
    "                self.layers = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    get_activation(activations[0]),\n",
    "                    \n",
    "                    nn.Conv2d(mid_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "            case 'Bottleneck':\n",
    "                mid_channels = mid_channels or in_channels // 2\n",
    "                out_channels = out_channels or in_channels\n",
    "                \n",
    "                self.layers = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, mid_channels, kernel_size=1),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    get_activation(activations[0]),\n",
    "                    \n",
    "                    nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                    nn.BatchNorm2d(mid_channels),\n",
    "                    get_activation(activations[1]),\n",
    "                    \n",
    "                    nn.Conv2d(mid_channels, out_channels, kernel_size=1),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "            case _:\n",
    "                raise Exception('Unsupported block type')\n",
    "                \n",
    "        self.activation = get_activation(activations[-1])\n",
    "        \n",
    "        self.sc_pool = None\n",
    "        self.sc_scale = None\n",
    "        \n",
    "        if stride > 1:\n",
    "            self.sc_pool = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if in_channels != out_channels:\n",
    "            self.sc_scale = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.layers(x)\n",
    "                \n",
    "        if self.sc_pool:\n",
    "            shortcut = self.sc_pool(shortcut)\n",
    "        if self.sc_scale:\n",
    "            shortcut = self.sc_scale(shortcut)\n",
    "        \n",
    "        x = self.activation(shortcut + x)\n",
    "        return x\n",
    "print(ResidualBlock('Standard', 2, 4, stride=2)(torch.rand(1, 2, 10, 10)).shape)\n",
    "print(ResidualBlock('Bottleneck', 2, 4, stride=2)(torch.rand(1, 2, 10, 10)).shape)\n",
    "print(ResidualBlock('Standard', 2)(torch.rand(1, 2, 10, 10)).shape)\n",
    "print(ResidualBlock('Bottleneck', 2)(torch.rand(1, 2, 10, 10)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualLayer(\n",
       "  (block1): ResidualBlock(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(2, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (sc_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (sc_scale): Conv2d(2, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (same_blocks): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Tanh()\n",
       "        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, block_type: str, n_blocks, in_channels: int, out_channels: Optional[int] = None, activations: List[str] | str = 'relu'):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        \n",
    "        if not isinstance(activations, List):\n",
    "            activations = [activations] * n_blocks\n",
    "        if len(activations) != n_blocks:\n",
    "            raise Exception('Not enough activations')\n",
    "        \n",
    "        out_channels = out_channels or in_channels * 2\n",
    "        \n",
    "        self.block1 = ResidualBlock(block_type, in_channels, out_channels, stride=2, activations=activations[0])\n",
    "        self.same_blocks = nn.Sequential(*[ResidualBlock(block_type, out_channels, activations=activations[i]) for i in range(1, n_blocks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.same_blocks(x)\n",
    "        return x\n",
    "ResidualLayer('Standard', 2, 2, activations=['relu', ['tanh', 'relu']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
